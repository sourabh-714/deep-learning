{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting install\n",
      "  Downloading install-1.3.4-py3-none-any.whl (3.1 kB)\n",
      "Collecting nltk\n",
      "  Downloading nltk-3.5.zip (1.4 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.4 MB 4.1 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied, skipping upgrade: click in /Users/sourabhsharma/anaconda3/lib/python3.7/site-packages (from nltk) (7.0)\n",
      "Requirement already satisfied, skipping upgrade: joblib in /Users/sourabhsharma/anaconda3/lib/python3.7/site-packages (from nltk) (0.14.1)\n",
      "Collecting regex\n",
      "  Downloading regex-2021.3.17-cp37-cp37m-macosx_10_9_x86_64.whl (285 kB)\n",
      "\u001b[K     |████████████████████████████████| 285 kB 4.7 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied, skipping upgrade: tqdm in /Users/sourabhsharma/anaconda3/lib/python3.7/site-packages (from nltk) (4.42.0)\n",
      "Building wheels for collected packages: nltk\n",
      "  Building wheel for nltk (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for nltk: filename=nltk-3.5-py3-none-any.whl size=1434678 sha256=84ebe0a8c995e3614c52726b26690edebde37e433c29e3515d89f27775206603\n",
      "  Stored in directory: /Users/sourabhsharma/Library/Caches/pip/wheels/45/6c/46/a1865e7ba706b3817f5d1b2ff7ce8996aabdd0d03d47ba0266\n",
      "Successfully built nltk\n",
      "\u001b[31mERROR: preprocessing 0.1.13 has requirement nltk==3.2.4, but you'll have nltk 3.5 which is incompatible.\u001b[0m\n",
      "Installing collected packages: install, regex, nltk\n",
      "  Attempting uninstall: nltk\n",
      "    Found existing installation: nltk 3.2.4\n",
      "    Uninstalling nltk-3.2.4:\n",
      "      Successfully uninstalled nltk-3.2.4\n",
      "Successfully installed install-1.3.4 nltk-3.5 regex-2021.3.17\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install -U install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Downloader.download of <nltk.downloader.Downloader object at 0x7fb31d2537b8>>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import sent_tokenize\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', 'How', 'are', 'YOu', '.', 'Iam', 'FIne']\n",
      "['Hello How are YOu .', 'Iam FIne']\n"
     ]
    }
   ],
   "source": [
    "# word tikenization\n",
    "text=\"Hello How are YOu . Iam FIne\"\n",
    "print(word_tokenize(text))\n",
    "print(sent_tokenize(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this is in caps\n"
     ]
    }
   ],
   "source": [
    "text1=\"THIS IS IN CAPS\"\n",
    "print(text1.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "print(stopwords.words('english'))\n",
    "\n",
    "# random sentecnce with lot of stop words\n",
    "sample_text = \"Oh man, this is pretty cool. We will do more such things.\"\n",
    "text_tokens = word_tokenize(sample_text)\n",
    "\n",
    "tokens_without_sw = [word for word in text_tokens if not word in stopwords.words('english')]\n",
    "\n",
    "print(text_tokens)\n",
    "print(tokens_without_sw)\n",
    "view raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'world', 'dependent', 'experience', 'great', 'people']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stopword=stopwords.words('english')\n",
    "text=\"The world is dependent on the experience of the great people\"\n",
    "word_tokens=nltk.word_tokenize(text)\n",
    "removing_stopwords=[x for x in word_tokens if x not in stopword]\n",
    "print(removing_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['αλλα', 'αν', 'αντι', 'απο', 'αυτα', 'αυτεσ', 'αυτη', 'αυτο', 'αυτοι', 'αυτοσ', 'αυτουσ', 'αυτων', 'αἱ', 'αἳ', 'αἵ', 'αὐτόσ', 'αὐτὸς', 'αὖ', 'γάρ', 'γα', 'γα^', 'γε', 'για', 'γοῦν', 'γὰρ', \"δ'\", 'δέ', 'δή', 'δαί', 'δαίσ', 'δαὶ', 'δαὶς', 'δε', 'δεν', \"δι'\", 'διά', 'διὰ', 'δὲ', 'δὴ', 'δ’', 'εαν', 'ειμαι', 'ειμαστε', 'ειναι', 'εισαι', 'ειστε', 'εκεινα', 'εκεινεσ', 'εκεινη', 'εκεινο', 'εκεινοι', 'εκεινοσ', 'εκεινουσ', 'εκεινων', 'ενω', 'επ', 'επι', 'εἰ', 'εἰμί', 'εἰμὶ', 'εἰς', 'εἰσ', 'εἴ', 'εἴμι', 'εἴτε', 'η', 'θα', 'ισωσ', 'κ', 'καί', 'καίτοι', 'καθ', 'και', 'κατ', 'κατά', 'κατα', 'κατὰ', 'καὶ', 'κι', 'κἀν', 'κἂν', 'μέν', 'μή', 'μήτε', 'μα', 'με', 'μεθ', 'μετ', 'μετά', 'μετα', 'μετὰ', 'μη', 'μην', 'μἐν', 'μὲν', 'μὴ', 'μὴν', 'να', 'ο', 'οι', 'ομωσ', 'οπωσ', 'οσο', 'οτι', 'οἱ', 'οἳ', 'οἷς', 'οὐ', 'οὐδ', 'οὐδέ', 'οὐδείσ', 'οὐδεὶς', 'οὐδὲ', 'οὐδὲν', 'οὐκ', 'οὐχ', 'οὐχὶ', 'οὓς', 'οὔτε', 'οὕτω', 'οὕτως', 'οὕτωσ', 'οὖν', 'οὗ', 'οὗτος', 'οὗτοσ', 'παρ', 'παρά', 'παρα', 'παρὰ', 'περί', 'περὶ', 'ποια', 'ποιεσ', 'ποιο', 'ποιοι', 'ποιοσ', 'ποιουσ', 'ποιων', 'ποτε', 'που', 'ποῦ', 'προ', 'προσ', 'πρόσ', 'πρὸ', 'πρὸς', 'πως', 'πωσ', 'σε', 'στη', 'στην', 'στο', 'στον', 'σόσ', 'σύ', 'σύν', 'σὸς', 'σὺ', 'σὺν', 'τά', 'τήν', 'τί', 'τίς', 'τίσ', 'τα', 'ταῖς', 'τε', 'την', 'τησ', 'τι', 'τινα', 'τις', 'τισ', 'το', 'τοί', 'τοι', 'τοιοῦτος', 'τοιοῦτοσ', 'τον', 'τοτε', 'του', 'τούσ', 'τοὺς', 'τοῖς', 'τοῦ', 'των', 'τό', 'τόν', 'τότε', 'τὰ', 'τὰς', 'τὴν', 'τὸ', 'τὸν', 'τῆς', 'τῆσ', 'τῇ', 'τῶν', 'τῷ', 'ωσ', \"ἀλλ'\", 'ἀλλά', 'ἀλλὰ', 'ἀλλ’', 'ἀπ', 'ἀπό', 'ἀπὸ', 'ἀφ', 'ἂν', 'ἃ', 'ἄλλος', 'ἄλλοσ', 'ἄν', 'ἄρα', 'ἅμα', 'ἐάν', 'ἐγώ', 'ἐγὼ', 'ἐκ', 'ἐμόσ', 'ἐμὸς', 'ἐν', 'ἐξ', 'ἐπί', 'ἐπεὶ', 'ἐπὶ', 'ἐστι', 'ἐφ', 'ἐὰν', 'ἑαυτοῦ', 'ἔτι', 'ἡ', 'ἢ', 'ἣ', 'ἤ', 'ἥ', 'ἧς', 'ἵνα', 'ὁ', 'ὃ', 'ὃν', 'ὃς', 'ὅ', 'ὅδε', 'ὅθεν', 'ὅπερ', 'ὅς', 'ὅσ', 'ὅστις', 'ὅστισ', 'ὅτε', 'ὅτι', 'ὑμόσ', 'ὑπ', 'ὑπέρ', 'ὑπό', 'ὑπὲρ', 'ὑπὸ', 'ὡς', 'ὡσ', 'ὥς', 'ὥστε', 'ὦ', 'ᾧ']\n"
     ]
    }
   ],
   "source": [
    "#print all the greek stopwords from the corpus\n",
    "greek=stopwords.words('greek')\n",
    "greek1=[ i for i in greek]\n",
    "print(greek1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['_LazyModule__lazymodule_globals', '_LazyModule__lazymodule_import', '_LazyModule__lazymodule_init', '_LazyModule__lazymodule_loaded', '_LazyModule__lazymodule_locals', '_LazyModule__lazymodule_name', '__class__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattr__', '__getattribute__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__module__', '__name__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__']\n"
     ]
    }
   ],
   "source": [
    "# exploring all the corpus \n",
    "import nltk.corpus\n",
    "dir(nltk.corpus)\n",
    "print(dir(nltk.corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to\n",
      "[nltk_data]     /Users/sourabhsharma/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/brown.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#BRown Corpora\n",
    "from nltk.corpus import brown\n",
    "nltk.download('brown')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'Fulton', 'County', 'Grand', 'Jury', 'said', ...]\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import brown\n",
    "x=brown.words()\n",
    "print(brown.words())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('this', 'test', 'sentence', 'has')\n",
      "('test', 'sentence', 'has', 'eight')\n",
      "('sentence', 'has', 'eight', 'words')\n",
      "('has', 'eight', 'words', 'in')\n",
      "('eight', 'words', 'in', 'it')\n"
     ]
    }
   ],
   "source": [
    "#obtaining all n grams for n =4\n",
    "from nltk import ngrams\n",
    "test = 'this test sentence has eight words in it'\n",
    "tokenize = nltk.word_tokenize(test)\n",
    "grams=ngrams(tokenize, 4)\n",
    "for x in grams:\n",
    "    print(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
